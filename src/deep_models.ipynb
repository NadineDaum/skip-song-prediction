{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "289139ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19994, 19095, 16696)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.sequence_dataset import SessionDataset, collate_fn\n",
    "\n",
    "# Load data\n",
    "train = pd.read_parquet(\"data/train.parquet\")\n",
    "val   = pd.read_parquet(\"data/val.parquet\")\n",
    "test  = pd.read_parquet(\"data/test.parquet\")\n",
    "\n",
    "feature_cols = [\n",
    "    \"position\", \"popularity\", \"acousticness\", \n",
    "    \"danceability\", \"energy\", \"tempo\", \"duration_sec\"\n",
    "]\n",
    "\n",
    "target_col = \"skip\"\n",
    "\n",
    "train_ds = SessionDataset(train, feature_cols, target_col, max_len=50)\n",
    "val_ds   = SessionDataset(val,   feature_cols, target_col, max_len=50)\n",
    "test_ds  = SessionDataset(test,  feature_cols, target_col, max_len=50)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_ds, batch_size=128, collate_fn=collate_fn)\n",
    "\n",
    "len(train_ds), len(val_ds), len(test_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a3e39e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_x: torch.Size([64, 50, 7])\n",
      "batch_y: torch.Size([64, 50])\n",
      "lengths: tensor([50, 50, 50, 50, 50, 50, 50, 50, 50, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/GitHub 2025/skip-song-prediction/src/sequence_dataset.py:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xs.append(torch.tensor(X))\n",
      "/Users/GitHub 2025/skip-song-prediction/src/sequence_dataset.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ys.append(torch.tensor(y))\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "batch_x, batch_y, lengths = next(iter(train_loader))\n",
    "\n",
    "print(\"batch_x:\", batch_x.shape)   # (B, T, F)\n",
    "print(\"batch_y:\", batch_y.shape)   # (B, T)\n",
    "print(\"lengths:\", lengths[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b98f568",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, num_features, hidden_size=64):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=num_features,\n",
    "            hidden_size=hidden_size,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            x, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        output, _ = self.lstm(packed)\n",
    "        padded, _ = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)\n",
    "\n",
    "        logits = self.fc(padded).squeeze(-1)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d29be62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for x, y, lengths in loader:\n",
    "        x, y, lengths = x.to(device), y.to(device), lengths.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x, lengths)\n",
    "        loss = loss_fn(logits, y.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "174bd224",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_auc(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y, lengths in loader:\n",
    "            x, y, lengths = x.to(device), y.to(device), lengths.to(device)\n",
    "            logits = model(x, lengths)\n",
    "            probs = torch.sigmoid(logits)\n",
    "\n",
    "            all_preds.append(probs.cpu().numpy().reshape(-1))\n",
    "            all_labels.append(y.cpu().numpy().reshape(-1))\n",
    "\n",
    "    preds = np.concatenate(all_preds)\n",
    "    labels = np.concatenate(all_labels)\n",
    "\n",
    "    return roc_auc_score(labels, preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1099bc20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LSTM] Epoch 1: loss=0.2439, val AUC=0.9720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/GitHub 2025/skip-song-prediction/src/sequence_dataset.py:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xs.append(torch.tensor(X))\n",
      "/Users/GitHub 2025/skip-song-prediction/src/sequence_dataset.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ys.append(torch.tensor(y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LSTM] Epoch 2: loss=0.2096, val AUC=0.9722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/GitHub 2025/skip-song-prediction/src/sequence_dataset.py:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xs.append(torch.tensor(X))\n",
      "/Users/GitHub 2025/skip-song-prediction/src/sequence_dataset.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ys.append(torch.tensor(y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LSTM] Epoch 3: loss=0.2094, val AUC=0.9722\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = LSTMModel(len(feature_cols)).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "lstm_auc = 0\n",
    "for epoch in range(3):\n",
    "    loss = train_one_epoch(model, train_loader, optimizer, loss_fn, device)\n",
    "    auc = evaluate_auc(model, val_loader, device)\n",
    "    lstm_auc = auc\n",
    "    print(f\"[LSTM] Epoch {epoch+1}: loss={loss:.4f}, val AUC={auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fbba9a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(\n",
    "            input_dim,\n",
    "            hidden_dim,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        output, hidden = self.gru(packed)\n",
    "        hidden = hidden[-1]  # last GRU layer\n",
    "        out = self.fc(hidden).squeeze(1)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2bcc7d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            x, lengths.cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        out_packed, _ = self.gru(packed)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(out_packed, batch_first=True)\n",
    "\n",
    "        logits = self.fc(out).squeeze(-1)   # (batch, seq_len)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d1ace61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GRU...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/GitHub 2025/skip-song-prediction/src/sequence_dataset.py:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xs.append(torch.tensor(X))\n",
      "/Users/GitHub 2025/skip-song-prediction/src/sequence_dataset.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ys.append(torch.tensor(y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GRU] Epoch 1: loss=0.2424, val AUC=0.9723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/GitHub 2025/skip-song-prediction/src/sequence_dataset.py:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xs.append(torch.tensor(X))\n",
      "/Users/GitHub 2025/skip-song-prediction/src/sequence_dataset.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ys.append(torch.tensor(y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GRU] Epoch 2: loss=0.2096, val AUC=0.9721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/GitHub 2025/skip-song-prediction/src/sequence_dataset.py:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xs.append(torch.tensor(X))\n",
      "/Users/GitHub 2025/skip-song-prediction/src/sequence_dataset.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ys.append(torch.tensor(y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GRU] Epoch 3: loss=0.2094, val AUC=0.9721\n"
     ]
    }
   ],
   "source": [
    "gru = GRUModel(len(feature_cols)).to(device)\n",
    "optimizer = torch.optim.Adam(gru.parameters(), lr=1e-3)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "print(\"Training GRU...\")\n",
    "for epoch in range(3):\n",
    "    loss = train_one_epoch(gru, train_loader, optimizer, loss_fn, device)\n",
    "    auc  = evaluate_auc(gru, val_loader, device)\n",
    "    print(f\"[GRU] Epoch {epoch+1}: loss={loss:.4f}, val AUC={auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "417f7e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, kernel_size=3):\n",
    "        super().__init__()\n",
    "\n",
    "        padding = kernel_size // 2  # keep sequence length\n",
    "\n",
    "        self.conv1 = nn.Conv1d(input_dim, hidden_dim, kernel_size, padding=padding)\n",
    "        self.relu  = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size, padding=padding)\n",
    "\n",
    "        self.classifier = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x, lengths=None):\n",
    "        # x: (batch, seq, features)\n",
    "        x = x.transpose(1, 2)            # â†’ (batch, features, seq)\n",
    "\n",
    "        h = self.relu(self.conv1(x))\n",
    "        h = self.relu(self.conv2(h))\n",
    "\n",
    "        h = h.transpose(1, 2)            # back to (batch, seq, hidden)\n",
    "        logits = self.classifier(h).squeeze(-1)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "64ec9e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Train CNN ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/GitHub 2025/skip-song-prediction/src/sequence_dataset.py:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xs.append(torch.tensor(X))\n",
      "/Users/GitHub 2025/skip-song-prediction/src/sequence_dataset.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ys.append(torch.tensor(y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CNN] Epoch 1: loss=0.3240, val AUC=0.9722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/GitHub 2025/skip-song-prediction/src/sequence_dataset.py:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xs.append(torch.tensor(X))\n",
      "/Users/GitHub 2025/skip-song-prediction/src/sequence_dataset.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ys.append(torch.tensor(y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CNN] Epoch 2: loss=0.2139, val AUC=0.9723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/GitHub 2025/skip-song-prediction/src/sequence_dataset.py:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xs.append(torch.tensor(X))\n",
      "/Users/GitHub 2025/skip-song-prediction/src/sequence_dataset.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ys.append(torch.tensor(y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CNN] Epoch 3: loss=0.2124, val AUC=0.9724\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Train CNN ===\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = CNNModel(input_dim=len(feature_cols)).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "for epoch in range(3):\n",
    "    loss = train_one_epoch(model, train_loader, optimizer, loss_fn, device)\n",
    "    auc  = evaluate_auc(model, val_loader, device)\n",
    "    print(f\"[CNN] Epoch {epoch+1}: loss={loss:.4f}, val AUC={auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bae7fe28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/GitHub 2025/skip-song-prediction/src/sequence_dataset.py:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xs.append(torch.tensor(X))\n",
      "/Users/GitHub 2025/skip-song-prediction/src/sequence_dataset.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ys.append(torch.tensor(y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LSTM] Epoch 1: loss=0.2561, val AUC=0.9721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/GitHub 2025/skip-song-prediction/src/sequence_dataset.py:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xs.append(torch.tensor(X))\n",
      "/Users/GitHub 2025/skip-song-prediction/src/sequence_dataset.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ys.append(torch.tensor(y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LSTM] Epoch 2: loss=0.2096, val AUC=0.9722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/GitHub 2025/skip-song-prediction/src/sequence_dataset.py:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xs.append(torch.tensor(X))\n",
      "/Users/GitHub 2025/skip-song-prediction/src/sequence_dataset.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ys.append(torch.tensor(y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LSTM] Epoch 3: loss=0.2093, val AUC=0.9721\n"
     ]
    }
   ],
   "source": [
    "model = LSTMModel(len(feature_cols)).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "lstm_auc = 0\n",
    "for epoch in range(3):\n",
    "    loss = train_one_epoch(model, train_loader, optimizer, loss_fn, device)\n",
    "    auc = evaluate_auc(model, val_loader, device)\n",
    "    lstm_auc = auc\n",
    "    print(f\"[LSTM] Epoch {epoch+1}: loss={loss:.4f}, val AUC={auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2d31eab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/GitHub 2025/skip-song-prediction/src/sequence_dataset.py:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xs.append(torch.tensor(X))\n",
      "/Users/GitHub 2025/skip-song-prediction/src/sequence_dataset.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ys.append(torch.tensor(y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GRU] Epoch 1: loss=0.2409, val AUC=0.9724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/GitHub 2025/skip-song-prediction/src/sequence_dataset.py:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xs.append(torch.tensor(X))\n",
      "/Users/GitHub 2025/skip-song-prediction/src/sequence_dataset.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ys.append(torch.tensor(y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GRU] Epoch 2: loss=0.2096, val AUC=0.9722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/GitHub 2025/skip-song-prediction/src/sequence_dataset.py:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xs.append(torch.tensor(X))\n",
      "/Users/GitHub 2025/skip-song-prediction/src/sequence_dataset.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ys.append(torch.tensor(y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GRU] Epoch 3: loss=0.2094, val AUC=0.9724\n"
     ]
    }
   ],
   "source": [
    "model = GRUModel(len(feature_cols)).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "gru_auc = 0\n",
    "for epoch in range(3):\n",
    "    loss = train_one_epoch(model, train_loader, optimizer, loss_fn, device)\n",
    "    auc = evaluate_auc(model, val_loader, device)\n",
    "    gru_auc = auc\n",
    "    print(f\"[GRU] Epoch {epoch+1}: loss={loss:.4f}, val AUC={auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "37323634",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/GitHub 2025/skip-song-prediction/src/sequence_dataset.py:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xs.append(torch.tensor(X))\n",
      "/Users/GitHub 2025/skip-song-prediction/src/sequence_dataset.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ys.append(torch.tensor(y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CNN] Epoch 1: loss=0.3042, val AUC=0.9705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/GitHub 2025/skip-song-prediction/src/sequence_dataset.py:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xs.append(torch.tensor(X))\n",
      "/Users/GitHub 2025/skip-song-prediction/src/sequence_dataset.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ys.append(torch.tensor(y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CNN] Epoch 2: loss=0.2139, val AUC=0.9722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/GitHub 2025/skip-song-prediction/src/sequence_dataset.py:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xs.append(torch.tensor(X))\n",
      "/Users/GitHub 2025/skip-song-prediction/src/sequence_dataset.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ys.append(torch.tensor(y))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CNN] Epoch 3: loss=0.2127, val AUC=0.9722\n"
     ]
    }
   ],
   "source": [
    "model = CNNModel(len(feature_cols)).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "cnn_auc = 0\n",
    "for epoch in range(3):\n",
    "    loss = train_one_epoch(model, train_loader, optimizer, loss_fn, device)\n",
    "    auc = evaluate_auc(model, val_loader, device)\n",
    "    cnn_auc = auc\n",
    "    print(f\"[CNN] Epoch {epoch+1}: loss={loss:.4f}, val AUC={auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f9a70df1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LSTM': np.float64(0.9721149184163527),\n",
       " 'GRU': np.float64(0.9723573694819648),\n",
       " 'CNN': np.float64(0.9722384597221317)}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = {\n",
    "    \"LSTM\": lstm_auc,\n",
    "    \"GRU\": gru_auc,\n",
    "    \"CNN\": cnn_auc\n",
    "}\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6fbb66bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GRU</td>\n",
       "      <td>0.972357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CNN</td>\n",
       "      <td>0.972238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>0.972115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>0.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogReg</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              model       auc\n",
       "3               GRU  0.972357\n",
       "4               CNN  0.972238\n",
       "2              LSTM  0.972115\n",
       "1  GradientBoosting  0.650000\n",
       "0            LogReg  0.600000"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Fill this from your baseline notebook\n",
    "logreg_auc = 0.60    # example, replace\n",
    "gb_auc     = 0.65    # example, replace\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    \"model\": [\"LogReg\", \"GradientBoosting\", \"LSTM\", \"GRU\", \"CNN\"],\n",
    "    \"auc\":   [logreg_auc, gb_auc, float(lstm_auc), float(gru_auc), float(cnn_auc)],\n",
    "})\n",
    "\n",
    "results_df.sort_values(\"auc\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d523355f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results_sorted = results_df.sort_values(\"auc\", ascending=False)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(results_sorted[\"model\"], results_sorted[\"auc\"])\n",
    "plt.ylabel(\"ROC AUC\")\n",
    "plt.title(\"Model Comparison: Skip Prediction\")\n",
    "plt.ylim(0.5, 1.0)\n",
    "plt.grid(axis=\"y\", alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e645b01e",
   "metadata": {},
   "source": [
    "**Model Comparison Interpretation**:\n",
    "\n",
    "The results show a clear performance gap between classical models and sequence-based deep learning models. Sequence models benefit from modeling how a user interacts with tracks over time (capturing dynamics such as streaks of skips, transitions between genres, or session-specific context). Classical models treat each event in isolation and therefore miss these temporal dependencies. The consistently high ROC-AUC scores for all three architectures indicate that skip prediction is fundamentally a sequential problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef90670",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": "skip-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
